# https://www.codementor.io/python/tutorial/data-science-python-pandas-r-dimensionality-reduction
# http://blog.yhat.com/posts/customer-segmentation-using-python.html
# excel file in google drive

%matplotlib inline
import pandas as pd
from dateutil.parser import parse
from datetime import datetime
import xlrd
from pandasql import sqldf
from pandas import ExcelWriter
from numbers import Number
import numpy as np
import pylab as pl
from scipy import stats
from sklearn.externals.six import StringIO
from sklearn import preprocessing
from sklearn import cluster, tree, decomposition
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import seaborn
import matplotlib.pyplot as plt
# import pydot

# Create a dataframe
# specify file path for windows users: target_file = r'C:\Users\k6o\aseanoutput.xlsx'
target_file1 = r'apsinstallbase_clustering1.xlsx'
xls_file1 = pd.read_excel(target_file1)
T1=pd.DataFrame(xls_file1)
T1.head()

# might need to normalize country mapping and industry mapping

num_list = ['Country Mapping', 'SW Industry Mapping', 'SW ', 'SW Office',
            'SW Pro', 'SW Premium', 'Simulation', 'Simulation Pro', 'Simulation Premium', 
            'Motion', 'Flow Simulation', 'PDMEnterprise', '3DVIA Composer', 'Electrical 2D'
            , 'Electrical 3D', 'Electrical Professional', 'SW Plastic Professional', 'SW Plastic Premium',
            'SW Plastic Advanced', 'SW Inspection']
            
T2 = T1[num_list]
            
# do PCA 
pca = PCA(n_components=2)
pca.fit(T2)
PCA(copy=True, n_components=2, whiten=False)
existing_2d = pca.transform(T2)
existing_df_2d = pd.DataFrame(existing_2d)
existing_df_2d.index = T2.index
existing_df_2d.columns = ['PC1','PC2']
existing_df_2d.head()
print(pca.explained_variance_ratio_)
#Explained variance
pca = PCA().fit(T2)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.show()

kmeans = KMeans(n_clusters=4)
clusters = kmeans.fit(existing_df_2d)
T1['cluster'] = pd.Series(clusters.labels_, index=T1.index)
existing_df_2d['cluster'] = pd.Series(clusters.labels_, index=T1.index)
T2['cluster'] = pd.Series(clusters.labels_, index=T1.index)


existing_df_2d.plot(
        kind='scatter',
        x='PC2',y='PC1',
        c=T1['cluster'].astype(np.float), 
        figsize=(16,8))

# plot by industry
df_is1 = T1[(T1['cluster'] == 1)]
df_is1.describe().transpose()
T1_industry = T1.groupby(['cluster', 'SW Industry'])['CSN'].count()
# T1_industry_ = pd.pivot_table(T1,columns=["cluster","SW Industry"],values=["CSN"],aggfunc='count')
T1_industrydf = pd.DataFrame(T1_industry).reset_index()
T1_industrydf = T1_industrydf.pivot(index='SW Industry', columns='cluster', values='CSN')
T1_industrydf.plot.bar(x="SW Industry", y="CSN", title=str('Count of Industry'), figsize=(16,8), subplots=True)


df_is0 = T1[(T1['cluster'] == 0)]
df_is0.describe().transpose()


df_is2 = T1[(T1['cluster'] == 2)]
df_is2.describe().transpose()


df_is3 = T1[(T1['cluster'] == 3)]
df_is3.describe().transpose()


fig, axes = plt.subplots(nrows=1, ncols=4)
for i, c in enumerate(T1_industrydf.columns):
    T1_industrydf[c].plot(kind='bar', ax=axes[i], figsize=(16, 12), title=c)

# correlation matrix

corr_table = T1[["Country Mapping", "SW Industry Mapping", "SW ", "SW Office", "SW Pro", "SW Premium", "Simulation", "Simulation Pro", 
                "Simulation Premium", "Motion", "Flow Simulation", "PDMEnterprise", "3DVIA Composer", "Electrical 2D", "Electrical 3D"
                , "Electrical Professional", "SW Plastic Professional", "SW Plastic Premium", "SW Plastic Advanced", 
                "SW Inspection", "cluster"]]
corr_df = corr_table.corr(method='pearson')

print("--------------- CREATE A HEATMAP ---------------")
# Create a mask to display only the lower triangle of the matrix (since it's mirrored around its 
# top-left to bottom-right diagonal).
mask = np.zeros_like(corr_df)
mask[np.triu_indices_from(mask)] = True
# Create the heatmap using seaborn library. 
# List if colormaps (parameter 'cmap') is available here: http://matplotlib.org/examples/color/colormaps_reference.html
seaborn.heatmap(corr_df, cmap='RdYlGn_r', vmax=1.0, vmin=-1.0 , mask = mask, linewidths=2.5)
 
# Show the plot we reorient the labels for each column and row to make them easier to read.
plt.yticks(rotation=0) 
plt.xticks(rotation=90) 
plt.show()

# export to excel

writer = ExcelWriter('installbaseOutput_clustering.xlsx')
T1.to_excel(writer, 'Full Table')
df_is0.to_excel(writer,'Cluster 0')
df_is1.to_excel(writer,'Cluster 1')
df_is2.to_excel(writer, 'Cluster 2')
df_is3.to_excel(writer, 'Cluster 3')
writer.save()
